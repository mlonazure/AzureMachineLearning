{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Data: Transfer Data from Blob Storage to Azure SQL Database using an existing Azure Data Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "[Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace%28class%29?view=azure-ml-py) \n",
    "Connect to your Azure Machine Learning service Workspace. Note that if you are using a Compute Instance the config file is already there for the Workspace you're within, otherwise you will need to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "mlonazure_ws = Workspace.from_config()\n",
    "\n",
    "mlonazure_ds = mlonazure_ws.get_default_datastore()\n",
    "\n",
    "print('Workspace Name: ' + mlonazure_ws.name, \n",
    "      'Resource Group: ' + mlonazure_ws.resource_group,\n",
    "      'Default Storage Account Name: ' + mlonazure_ds.account_name,\n",
    "      'AzureML Core Version: ' + azureml.core.VERSION,\n",
    "      sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "[DataReference Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.data_reference.datareference?view=azure-ml-py) Create a reference to the blob storage. Note that in the path_on_datastore you can use wildcards for multipe files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "blob_data_ref = DataReference(datastore = mlonazure_ds, \n",
    "                  data_reference_name='blobref', \n",
    "                  path_on_datastore='MyDatasets/OJSales_All/*.csv', \n",
    "                  mode='mount', \n",
    "                  path_on_compute=None, \n",
    "                  overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "[SqlDataReference Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.sql_data_reference.sqldatareference?view=azure-ml-py) Create a reference to the SQL Datastore registered in (01. Transfer Data Configuration) and the specific table to write to. \n",
    "\n",
    "**Note** There is currently a bug in the SDK that writes to a table named 'dummy' and not the sql_table name passed here. Therefore, make sure you create the table 'dummy' (I know, I know...) and provide the Service Princple approperiate permissions to read/write to it.\n",
    "\n",
    "**Note** The DataTransferStep assumes your data has a header and uses the first row to do a column mapping with your table's columns, the names need to match!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "\n",
    "mlonazuresql_ds = Datastore(workspace=mlonazure_ws,\n",
    "                            name='azuresqldb_datastore')\n",
    "\n",
    "sql_query_data_ref = SqlDataReference(\n",
    "                    datastore = mlonazuresql_ds, \n",
    "                    data_reference_name='sqlreference', \n",
    "                    sql_table='stageOJSales', \n",
    "                    sql_query=None, \n",
    "                    sql_stored_procedure=None, \n",
    "                    sql_stored_procedure_params=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "[DataFactoryCompute Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.datafactory.datafactorycompute?view=azure-ml-py) Simply connect to the datafactory that was attached in the 01. Transfer Data Coniguration Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import DataFactoryCompute\n",
    "\n",
    "adfcompute = 'mlonazure-adf'\n",
    "adf_compute = DataFactoryCompute(workspace=mlonazure_ws, name=adfcompute)\n",
    "adf_compute.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "[DataTransferStep class] (https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.datatransferstep?view=azure-ml-py) Create a DataTransferStep providing it the blob reference, sql reference and the adf compute from above. \n",
    "\n",
    "**Note** allow_reuse is set to false so that the step always runs regardless if the configuration of the step is changed or not. If this is set to true then you might run your pipline and not see anything happen in ADF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import DataTransferStep\n",
    "\n",
    "datatransferstep_name = 'transfer_blob_to_sql'\n",
    "\n",
    "data_transfer_step = DataTransferStep(name = datatransferstep_name, \n",
    "                         source_data_reference=blob_data_ref, \n",
    "                         destination_data_reference=sql_query_data_ref, \n",
    "                         compute_target=adf_compute, \n",
    "                         source_reference_type='file', \n",
    "                         #destination_reference_type=None, \n",
    "                         allow_reuse=False) #Allows reuse of prev results if settings are the same.\n",
    "\n",
    "print(\"Data transfer step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "[Pipeline class](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py) Create the pipeline with only one step which is the datatransferstep from above.\n",
    "\n",
    "**Note** that the pipeline can have many steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "datatransfer_pipeline = Pipeline(workspace=mlonazure_ws, \n",
    "         steps=[data_transfer_step], \n",
    "         description='Transfer blob data to sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "[Experiment Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment%28class%29?view=azure-ml-py) Create an Experiment, check Studio to see your experiment is there now without any runs\n",
    "\n",
    "Submit the experiment and wait for completion. While you wait for completion, go to Studio and click on Pipleines, you will see your pipeline run from there. \n",
    "\n",
    "**Note** Because this run is using ADF, you can go into the ADF you have and look at Monitor to see it. Errors may not come back up to Studio and therefore, see the errors in ADF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment \n",
    "\n",
    "exp = Experiment(workspace = mlonazure_ws, name=\"DataTransfer_BlobtoSQL\")\n",
    "\n",
    "exp_pipelinerun = exp.submit(datatransfer_pipeline)\n",
    "\n",
    "exp_pipelinerun.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "Publish the pipeline so that you have a Pipeline endpoint that can be called directly in the future. See Studio -> End Points -> Pipeline endpoints. Click on the endpoint and then you can submit there or you can use ADF or Azure Devops to call it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatransfer_pipeline.publish(\n",
    "        name='Transfer OJSales Data', \n",
    "        description='Transfer OJSales Data from Blob Storage to SQL Table', \n",
    "        version=None, \n",
    "        continue_on_step_failure=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
